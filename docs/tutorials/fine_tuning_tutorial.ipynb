{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d80f419",
   "metadata": {},
   "source": [
    "# Fine-tuning AntiBERTy to Predict S Protein Binding\n",
    "\n",
    "This Colab notebook shows how to fine-tune the **AntiBERTy** antibody language model to predict whether an antibody sequence binds the SARS-CoV-2 Spike (S) protein.\n",
    "\n",
    "It is adapted from a script to perform supervised fine-tuning of pre-trained antibody language models for antigen specificity prediction as published by [Wang. et al. 2025](https://doi.org/10.1371/journal.pcbi.1012153) that:\n",
    "- Loads antibody sequences and S-binding labels\n",
    "- Formats sequences for AntiBERTy\n",
    "- Uses a grouped, stratified cross-validation split\n",
    "- Fine-tunes AntiBERTy with Hugging Face `Trainer`\n",
    "- Evaluates with AUC, MCC, balanced accuracy, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## How to use this notebook\n",
    "\n",
    "1. Download a parquet file:\n",
    "   - e.g. `S_CDR3.parquet` from [figshare](https://figshare.com/articles/dataset/Fine-tuning_Pre-trained_Antibody_Language_Models_for_Antigen_Specificity_Prediction/25342924)\n",
    "\n",
    "2. Update `DATA_DIR` and `OUTPUT_DIR` below if needed.\n",
    "\n",
    "---\n",
    "\n",
    "> **Dataset assumptions**\n",
    "> - Columns:\n",
    ">   - `HL` or `H`: sequences (heavy + light vs heavy only)\n",
    ">   - `label`: includes antigen binding values like `\"S+\"`, `\"S1+\"`, `\"S2+\"` (positive) and others (negative)\n",
    ">   - `subject`: donor / study ID for grouped cross-validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb486424",
   "metadata": {},
   "source": [
    "## Install dependencies (run once per session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f4883f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q antiberty transformers datasets scikit-learn biopython pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60cd3c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf4eddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.3.0\n",
      "CUDA available: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    matthews_corrcoef, roc_auc_score,\n",
    "    average_precision_score, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from datasets import Dataset, DatasetDict, ClassLabel\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "import antiberty\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91d8518",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04ca3624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run ID: S_antiBERTy_HL_fine_tuning_CDR3\n",
      "Data dir: ../data/\n",
      "Output dir: ../models/\n"
     ]
    }
   ],
   "source": [
    "# Which column in the parquet to use as sequences\n",
    "MODEL_TYPE = \"HL\"  \n",
    "\n",
    "# Which dataset variant to use\n",
    "SEQUENCE_SCOPE = \"CDR3\"  \n",
    "\n",
    "# Path to your data directory\n",
    "DATA_DIR = \"../data/\" \n",
    "\n",
    "# Path where models and logs will be saved\n",
    "OUTPUT_DIR = \"../models/\"\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 64        \n",
    "LR = 1e-5              \n",
    "N_EPOCHS = 10          \n",
    "\n",
    "RANDOM_STATE_OUTER = 7 if SEQUENCE_SCOPE == \"CDR3\" else 9\n",
    "RANDOM_STATE_INNER = 1\n",
    "\n",
    "RUN_ID = f\"S_antiBERTy_{MODEL_TYPE}_fine_tuning_{SEQUENCE_SCOPE}\"\n",
    "print(\"Run ID:\", RUN_ID)\n",
    "print(\"Data dir:\", DATA_DIR)\n",
    "print(\"Output dir:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "756653c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/vast/palmer/home.mccleary/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/antiberty'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.path.realpath(antiberty.__file__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c78f55",
   "metadata": {},
   "source": [
    "# Helper functions: model loading, freezing, formatting, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00785585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_antiberty_paths():\n",
    "    \"\"\"Locate AntiBERTy model + vocab from the antiberty package.\"\"\"\n",
    "    project_path = os.path.dirname(os.path.realpath(antiberty.__file__))\n",
    "    trained_dir = os.path.join(project_path, \"trained_models\")\n",
    "    model_dir = os.path.join(trained_dir, \"AntiBERTy_md_smooth\")\n",
    "    vocab = os.path.join(trained_dir, \"vocab.txt\")\n",
    "    print(\"AntiBERTy model:\", model_dir)\n",
    "    print(\"AntiBERTy vocab:\", vocab)\n",
    "    return model_dir, vocab\n",
    "\n",
    "\n",
    "def load_antiberty_classifier(num_labels: int = 2):\n",
    "    \"\"\"Load AntiBERTy as a sequence-classification model + tokenizer.\"\"\"\n",
    "    model_dir, vocab = get_antiberty_paths()\n",
    "    tokenizer = transformers.BertTokenizer(\n",
    "        vocab_file=vocab,\n",
    "        do_lower_case=False\n",
    "    )\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_dir,\n",
    "        num_labels=num_labels\n",
    "    )\n",
    "    model.to(device)\n",
    "    size = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Model size: {size/1e6:.2f}M parameters\")\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def freeze_antiberty_layers(model, train_last_n_layers: int = 3):\n",
    "    \"\"\"Freeze embeddings and early encoder layers of AntiBERTy.\"\"\"\n",
    "    for p in model.bert.embeddings.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    total_layers = len(model.bert.encoder.layer)  # AntiBERTy has 8 layers\n",
    "    for layer in model.bert.encoder.layer[: total_layers - train_last_n_layers]:\n",
    "        for p in layer.parameters():\n",
    "            p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "\n",
    "def insert_space_every_other_except_cls(s: str) -> str:\n",
    "    \"\"\"Add spaces between residues, keeping [CLS] intact.\"\"\"\n",
    "    parts = s.split(\"[CLS]\")\n",
    "    spaced = [\" \".join(list(part)) for part in parts]\n",
    "    out = \" [CLS] \".join(spaced)\n",
    "    return \" \".join(out.split())\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Metrics callback for Hugging Face Trainer.\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "\n",
    "    return {\n",
    "        \"precision\": precision_score(labels, preds),\n",
    "        \"recall\": recall_score(labels, preds),\n",
    "        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\"),\n",
    "        \"apr\": average_precision_score(labels, probs),\n",
    "        \"balanced_accuracy\": balanced_accuracy_score(labels, preds),\n",
    "        \"auc\": roc_auc_score(labels, probs),\n",
    "        \"mcc\": matthews_corrcoef(labels, preds),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a75af2",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69f14b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CDR3 sequences...\n",
      "Total sequences: 15539\n",
      "Unique donors: 427\n",
      "Label counts: Counter({1: 8658, 0: 6881})\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 512 - 2  # AntiBERTy max length minus specials\n",
    "\n",
    "def load_data(scope: str = \"CDR3\", model_type: str = \"HL\"):\n",
    "    if scope == \"FULL\":\n",
    "        filename = \"S_FULL.parquet\"\n",
    "        print(\"Loading full-length sequences...\")\n",
    "    else:\n",
    "        filename = \"S_CDR3.parquet\"\n",
    "        print(\"Loading CDR3 sequences...\")\n",
    "\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Could not find {path}. Please check DATA_DIR and filename.\")\n",
    "\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    X = df[model_type].apply(lambda s: s[:MAX_LENGTH])\n",
    "    X = X.str.replace(\"<cls><cls>\", \"[CLS][CLS]\", regex=False)\n",
    "    X = X.apply(insert_space_every_other_except_cls)\n",
    "\n",
    "    y = np.isin(df[\"label\"], [\"S+\", \"S1+\", \"S2+\"]).astype(int)\n",
    "    groups = df[\"subject\"].values\n",
    "\n",
    "    print(f\"Total sequences: {len(X)}\")\n",
    "    print(f\"Unique donors: {len(np.unique(groups))}\")\n",
    "    print(\"Label counts:\", Counter(y))\n",
    "\n",
    "    return X, y, groups, df\n",
    "\n",
    "\n",
    "X, y, y_groups, raw_df = load_data(SEQUENCE_SCOPE, MODEL_TYPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df11d2bc",
   "metadata": {},
   "source": [
    "# Create train/val/test splits (StratifiedGroupKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8279936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using outer fold 1\n",
      "Outer train size: 12969, test size: 2570\n",
      "% positive train: 0.555, % positive test: 0.566\n",
      "Using inner fold 1 for train/val split\n",
      "Final sizes â€” train: 8423, val: 4546, test: 2570\n"
     ]
    }
   ],
   "source": [
    "outer_cv = StratifiedGroupKFold(\n",
    "    n_splits=4,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE_OUTER\n",
    ")\n",
    "\n",
    "inner_cv = StratifiedGroupKFold(\n",
    "    n_splits=3,\n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_STATE_INNER\n",
    ")\n",
    "\n",
    "# Use the first outer fold for this tutorial\n",
    "for fold_idx, (train_index, test_index) in enumerate(outer_cv.split(X, y, y_groups), start=1):\n",
    "    print(f\"Using outer fold {fold_idx}\")\n",
    "    X_train_all, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train_all, y_test = y[train_index], y[test_index]\n",
    "    y_groups_train = y_groups[train_index]\n",
    "    break\n",
    "\n",
    "print(f\"Outer train size: {len(X_train_all)}, test size: {len(X_test)}\")\n",
    "print(f\"% positive train: {np.mean(y_train_all):.3f}, % positive test: {np.mean(y_test):.3f}\")\n",
    "\n",
    "# Inner split to create validation set\n",
    "for inner_idx, (inner_train_index, val_index) in enumerate(\n",
    "    inner_cv.split(X_train_all, y_train_all, y_groups_train),\n",
    "    start=1\n",
    "):\n",
    "    print(f\"Using inner fold {inner_idx} for train/val split\")\n",
    "    X_train = X_train_all.iloc[inner_train_index]\n",
    "    y_train = y_train_all[inner_train_index]\n",
    "    X_val = X_train_all.iloc[val_index]\n",
    "    y_val = y_train_all[val_index]\n",
    "    break\n",
    "\n",
    "print(f\"Final sizes â€” train: {len(X_train)}, val: {len(X_val)}, test: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c972c",
   "metadata": {},
   "source": [
    "# Build Hugging Face Datasets & tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaeaa8a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AntiBERTy model: /vast/palmer/home.mccleary/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/antiberty/trained_models/AntiBERTy_md_smooth\n",
      "AntiBERTy vocab: /vast/palmer/home.mccleary/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/antiberty/trained_models/vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /vast/palmer/home.mccleary/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/antiberty/trained_models/AntiBERTy_md_smooth and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 25.76M parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8423/8423 [00:02<00:00, 3713.91 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4546/4546 [00:01<00:00, 4072.29 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2570/2570 [00:00<00:00, 3655.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame({\"sequence\": X_train.values, \"labels\": y_train})\n",
    "val_df   = pd.DataFrame({\"sequence\": X_val.values,   \"labels\": y_val})\n",
    "test_df  = pd.DataFrame({\"sequence\": X_test.values,  \"labels\": y_test})\n",
    "\n",
    "raw_datasets = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df.reset_index(drop=True)),\n",
    "    \"validation\": Dataset.from_pandas(val_df.reset_index(drop=True)),\n",
    "    \"test\": Dataset.from_pandas(test_df.reset_index(drop=True)),\n",
    "})\n",
    "\n",
    "model, tokenizer = load_antiberty_classifier(num_labels=2)\n",
    "model = freeze_antiberty_layers(model, train_last_n_layers=3)\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    encodings = tokenizer(\n",
    "        batch[\"sequence\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "    )\n",
    "    encodings[\"labels\"] = batch[\"labels\"]\n",
    "    return encodings\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"sequence\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7572c373",
   "metadata": {},
   "source": [
    "# Training setup (Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aea82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mw957/.conda/envs/bcrembed/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoints to: ../models/S_antiBERTy_HL_fine_tuning_CDR3_Fold_1\n"
     ]
    }
   ],
   "source": [
    "set_seed(1)\n",
    "\n",
    "FOLD_ID = 1\n",
    "OUT_PATH = os.path.join(OUTPUT_DIR, f\"{RUN_ID}_Fold_{FOLD_ID}\")\n",
    "os.makedirs(OUT_PATH, exist_ok=True)\n",
    "print(\"Saving checkpoints to:\", OUT_PATH)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUT_PATH,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    learning_rate=LR,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=N_EPOCHS,\n",
    "    warmup_ratio=0.0,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"auc\",\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fccb6d",
   "metadata": {},
   "source": [
    "## Train AntiBERTy (this can take a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c263f296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1320' max='1320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1320/1320 10:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Weighted</th>\n",
       "      <th>Apr</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Auc</th>\n",
       "      <th>Mcc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.672582</td>\n",
       "      <td>0.578891</td>\n",
       "      <td>0.859656</td>\n",
       "      <td>0.531055</td>\n",
       "      <td>0.643557</td>\n",
       "      <td>0.547432</td>\n",
       "      <td>0.602929</td>\n",
       "      <td>0.122046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.663600</td>\n",
       "      <td>0.663214</td>\n",
       "      <td>0.601800</td>\n",
       "      <td>0.775290</td>\n",
       "      <td>0.576286</td>\n",
       "      <td>0.666190</td>\n",
       "      <td>0.573953</td>\n",
       "      <td>0.625232</td>\n",
       "      <td>0.161951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.651500</td>\n",
       "      <td>0.656686</td>\n",
       "      <td>0.629101</td>\n",
       "      <td>0.705318</td>\n",
       "      <td>0.604632</td>\n",
       "      <td>0.679539</td>\n",
       "      <td>0.598380</td>\n",
       "      <td>0.640495</td>\n",
       "      <td>0.201339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.636300</td>\n",
       "      <td>0.654390</td>\n",
       "      <td>0.635659</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0.608874</td>\n",
       "      <td>0.689057</td>\n",
       "      <td>0.602942</td>\n",
       "      <td>0.647817</td>\n",
       "      <td>0.208721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.629200</td>\n",
       "      <td>0.652828</td>\n",
       "      <td>0.638046</td>\n",
       "      <td>0.689324</td>\n",
       "      <td>0.611427</td>\n",
       "      <td>0.693718</td>\n",
       "      <td>0.605542</td>\n",
       "      <td>0.652462</td>\n",
       "      <td>0.213864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.619700</td>\n",
       "      <td>0.656932</td>\n",
       "      <td>0.674221</td>\n",
       "      <td>0.570972</td>\n",
       "      <td>0.612711</td>\n",
       "      <td>0.698114</td>\n",
       "      <td>0.616782</td>\n",
       "      <td>0.656764</td>\n",
       "      <td>0.232928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.615900</td>\n",
       "      <td>0.651222</td>\n",
       "      <td>0.647674</td>\n",
       "      <td>0.668133</td>\n",
       "      <td>0.616673</td>\n",
       "      <td>0.702476</td>\n",
       "      <td>0.611817</td>\n",
       "      <td>0.660182</td>\n",
       "      <td>0.224564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.611800</td>\n",
       "      <td>0.651564</td>\n",
       "      <td>0.652422</td>\n",
       "      <td>0.640944</td>\n",
       "      <td>0.614947</td>\n",
       "      <td>0.704302</td>\n",
       "      <td>0.611670</td>\n",
       "      <td>0.661166</td>\n",
       "      <td>0.222945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.607600</td>\n",
       "      <td>0.651861</td>\n",
       "      <td>0.657959</td>\n",
       "      <td>0.644542</td>\n",
       "      <td>0.620488</td>\n",
       "      <td>0.705702</td>\n",
       "      <td>0.617381</td>\n",
       "      <td>0.662358</td>\n",
       "      <td>0.234290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.606600</td>\n",
       "      <td>0.651196</td>\n",
       "      <td>0.648607</td>\n",
       "      <td>0.670132</td>\n",
       "      <td>0.617949</td>\n",
       "      <td>0.706074</td>\n",
       "      <td>0.613061</td>\n",
       "      <td>0.662967</td>\n",
       "      <td>0.227118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'train_runtime': 626.7467,\n",
       " 'train_samples_per_second': 134.392,\n",
       " 'train_steps_per_second': 2.106,\n",
       " 'total_flos': 6568285780076400.0,\n",
       " 'train_loss': 0.6328125693581321,\n",
       " 'epoch': 10.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_result = trainer.train()\n",
    "train_result.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1407d0e",
   "metadata": {},
   "source": [
    "# Evaluate on held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26dda1e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics:\n",
      "test_loss: 0.6747\n",
      "test_precision: 0.6530\n",
      "test_recall: 0.6224\n",
      "test_f1_weighted: 0.6003\n",
      "test_apr: 0.6858\n",
      "test_balanced_accuracy: 0.5957\n",
      "test_auc: 0.6316\n",
      "test_mcc: 0.1903\n",
      "test_runtime: 8.7614\n",
      "test_samples_per_second: 293.3340\n",
      "test_steps_per_second: 4.6800\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_outputs = trainer.predict(tokenized_datasets[\"test\"])\n",
    "test_metrics = test_outputs.metrics\n",
    "\n",
    "print(\"Test metrics:\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bcrembed",
   "language": "python",
   "name": "bcrembed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
